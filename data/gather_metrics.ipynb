{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import warnings\n",
    "import os \n",
    "import openai\n",
    "import os\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib as mpl\n",
    "from scipy import stats\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "project_dir = os.path.abspath(\n",
    "    os.path.join(os.path.abspath(os.getcwd()), \"../\"))\n",
    "\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "from src.ranker.codex.models import CodexBasedModel\n",
    "from src.ranker.codex.data import CrossDataSetForCodex\n",
    "from src.ligen.loopinv import verify\n",
    "from src.ranker.ranker import Ranker as Ranker_new\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_times = {\n",
    "    \"text-embedding-ada-002\": 0.006271725654602051,\n",
    "    \"davinci-similarity\": 0.04804928112030029,\n",
    "}\n",
    "problem_base_dir = \"in_scope\"\n",
    "\n",
    "\n",
    "# exp_name = \"gpt-3.5\"\n",
    "exp_name = \"gpt-4\"\n",
    "# codex_model = \"davinci-similarity\"\n",
    "codex_model = \"text-embedding-ada-002\"\n",
    "\n",
    "\n",
    "if codex_model == 'ada':\n",
    "    model_shorthand = 'ada'\n",
    "elif codex_model == 'text-embedding-ada-002':\n",
    "    model_shorthand = 'ada_002'\n",
    "elif codex_model == 'davinci-similarity':\n",
    "    model_shorthand = 'davinci'\n",
    "\n",
    "exp_dir = f\"solutions/{exp_name}/details\"\n",
    "\n",
    "\n",
    "files = [f for f in os.listdir(exp_dir) if f.endswith(\".json\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_to_problem_ids = {}\n",
    "problem_id_to_fold = {}\n",
    "with open(f\"ranker_data/problem_id_to_fold.json\") as f:\n",
    "    data = json.load(f)\n",
    "    problem_id_to_fold = data\n",
    "    for k in data:\n",
    "        v = data[k]\n",
    "        if v not in fold_to_problem_ids:\n",
    "            fold_to_problem_ids[v] = []\n",
    "        fold_to_problem_ids[v].append(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(\n",
    "    text, \n",
    "    codex_model, \n",
    "):\n",
    "    sleep_time = .1\n",
    "    while True:\n",
    "        try:\n",
    "            emb = openai.Embedding.create(\n",
    "                input=[text], model=codex_model\n",
    "            )['data'][0]['embedding']\n",
    "            return emb\n",
    "        except Exception as e:\n",
    "            if 'Please reduce' in str(e):\n",
    "                text = text[:int(.9 * len(text))]\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert openai.api_key is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "problem_dir = f\"problems/{problem_base_dir}\"\n",
    "fold_data_path = f\"aggregated_results_with_time/{exp_name}-data.json\"\n",
    "if os.path.exists(fold_data_path):\n",
    "    data = json.load(open(fold_data_path))\n",
    "    data_for_ranker = data['data_for_ranker']\n",
    "    solved_data = data['solved_data']\n",
    "    fold_data = data['fold_data']\n",
    "else:\n",
    "    data_for_ranker = []\n",
    "    solved_data = []\n",
    "    fold_data = {\n",
    "        \"fold_0\": [], \"fold_1\": [], \"fold_2\": [], \"fold_3\": [], \"fold_4\": [],\n",
    "    }\n",
    "    def verification_time(inv_code, problem_path):\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # verify(inv_code, problem_path)\n",
    "            # Call the verifier for getting the time.\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        end_time = time.time()\n",
    "        return float(end_time - start_time)\n",
    "\n",
    "    for f in tqdm(files):\n",
    "        d = json.load(open(f\"{exp_dir}/{f}\"))\n",
    "        problem_name = d['problem']\n",
    "        problem_path = f\"{problem_dir}/{problem_name}.sl\"\n",
    "        problem = open(problem_path, \"r\").read()\n",
    "        invariants = d['generated_invariants']\n",
    "        data = {\n",
    "            \"problem_path\": problem_path,\n",
    "            \"problem_name\": problem_name,\n",
    "            \"problem\": problem,\n",
    "            \"verified\": d['verified'],\n",
    "            \"invariants\": [\n",
    "                {\n",
    "                    \"code\": i['inv'],\n",
    "                    \"verified\": i['verified'],\n",
    "                    'call_to_z3': i['feedback']['call_to_z3'],\n",
    "                    'duration': i['duration'],\n",
    "                    'ver_time': verification_time(\n",
    "                        i['inv'], problem_path\n",
    "                    ) if d['verified'] else 0,\n",
    "                } for i in invariants\n",
    "            ]\n",
    "        }\n",
    "        data_for_ranker.append(data)\n",
    "        if d['verified']:\n",
    "            solved_data.append(data)\n",
    "            assert any([i['verified'] for i in invariants])\n",
    "            if problem_name in problem_id_to_fold:\n",
    "                fold = problem_id_to_fold[problem_name]\n",
    "                fold_data[fold].append(data)\n",
    "            else:\n",
    "                for fold in [\"fold_0\", \"fold_1\", \"fold_2\", \"fold_3\", \"fold_4\"]:\n",
    "                    fold_data[fold].append(data)\n",
    "\n",
    "print(len(data_for_ranker), len(solved_data), len(fold_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "os.makedirs(\"aggregated_results_with_time\", exist_ok=True)\n",
    "def print_results(df, model):\n",
    "    df.to_csv(f\"aggregated_results_with_time/{model}-{exp_name}-results.csv\")\n",
    "if not os.path.exists(fold_data_path):\n",
    "    with open(fold_data_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"data_for_ranker\": data_for_ranker,\n",
    "            \"solved_data\": solved_data,\n",
    "            \"fold_data\": fold_data,\n",
    "        }, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_file_name = f\"{exp_name}_embeddings_{model_shorthand}.json\"\n",
    "print(emb_file_name)\n",
    "print(os.path.exists(emb_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_embeddings = dict()\n",
    "code_set = set()\n",
    "for f in fold_data:\n",
    "    for d in fold_data[f]:\n",
    "        code_set.add(d['problem'])\n",
    "        for i in d['invariants']:\n",
    "            code_set.add(i['code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_recompute = False\n",
    "if force_recompute or not os.path.exists(emb_file_name):\n",
    "    for code in tqdm(code_set):\n",
    "        if code not in cached_embeddings:\n",
    "            embedding = get_embedding(code, codex_model)\n",
    "            cached_embeddings[code] = embedding\n",
    "    with open(emb_file_name, \"w\") as f:\n",
    "        json.dump(cached_embeddings, f)\n",
    "        f.close()\n",
    "else:\n",
    "    print(\"Loading cached embeddings\")\n",
    "    cached_embeddings = json.load(open(emb_file_name, \"r\"))\n",
    "\n",
    "\n",
    "not_found_codes = [\n",
    "    code for code in code_set if code not in cached_embeddings\n",
    "]\n",
    "print(len(not_found_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.\n",
    "\n",
    "hidden_dim = CrossDataSetForCodex.get_dimension(codex_model)\n",
    "model = CodexBasedModel(\n",
    "    hidden_dim=hidden_dim,\n",
    "    model_name=codex_model,\n",
    "    alpha=alpha\n",
    ")\n",
    "model_specific_arguments_for_ranker = {\n",
    "    \"model_name\": codex_model,\n",
    "    \"no_train_rank\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_embedding_time = embedding_times[codex_model]\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    new_tokens = []\n",
    "    for t in tokens:\n",
    "        new_tokens.extend(nltk.wordpunct_tokenize(t))\n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "def similarity_based_ranking(_data):\n",
    "    ranks = []\n",
    "    z3_calls = []\n",
    "    verification_times = []\n",
    "    ranking_times = []\n",
    "    for di in tqdm(range(len(_data))):\n",
    "        d = _data[di]\n",
    "        z3, inv_count = 0, 0\n",
    "        time_count = 0\n",
    "        all_codes = [d['problem']] + [i['code'] for i in d['invariants']]\n",
    "        start = time.time()\n",
    "        all_codes = [tokenize(c) for c in all_codes]\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectors = vectorizer.fit_transform(all_codes)\n",
    "        vectors = vectors.toarray()\n",
    "        problem_vector = vectors[0]\n",
    "        inv_vectors = vectors[1:]\n",
    "        similarities = []\n",
    "        for i in inv_vectors:\n",
    "            similarities.append(np.dot(i, problem_vector))\n",
    "        sorted_indices = np.argsort(similarities)[::-1]\n",
    "        reranked_invariants = [\n",
    "            d['invariants'][i] for i in sorted_indices\n",
    "        ]\n",
    "        end = time.time()\n",
    "        ranking_times.append(end - start)\n",
    "        for i in reranked_invariants:\n",
    "            z3 += i['call_to_z3']\n",
    "            inv_count += 1\n",
    "            elapsed_time = i['ver_time']\n",
    "            time_count += elapsed_time\n",
    "            if i['verified']:\n",
    "                break\n",
    "        z3_calls.append(z3)\n",
    "        ranks.append(inv_count)\n",
    "        verification_times.append(time_count)\n",
    "    return (\n",
    "        ranks, z3_calls, verification_times, \n",
    "        [0] * len(verification_times), \n",
    "        ranking_times\n",
    "    )\n",
    "\n",
    "def randomly_rank_invariants(data, num_random_rankings):\n",
    "    ranks = []\n",
    "    z3_calls = []\n",
    "    times = []\n",
    "    for d in data:\n",
    "        pranks = []\n",
    "        z3s = []\n",
    "        ptimes = []\n",
    "        for _ in range(num_random_rankings):\n",
    "            z3 = 0\n",
    "            time_count = 0\n",
    "            invariants = copy(d['invariants'])\n",
    "            np.random.shuffle(invariants)\n",
    "            for iidx, i in enumerate(invariants):\n",
    "                if 'ver_time' in i:\n",
    "                    time_count += i['ver_time']\n",
    "                else:\n",
    "                    start = time.time()\n",
    "                    verify(\n",
    "                        inv=i['code'], \n",
    "                        problem_path=d['problem_path'],\n",
    "                    )\n",
    "                    end = time.time()\n",
    "                    time_count += (end - start)\n",
    "                z3 += i['call_to_z3']\n",
    "                if i['verified']:\n",
    "                    pranks.append(iidx + 1)\n",
    "                    break\n",
    "            z3s.append(z3)\n",
    "            ptimes.append(time_count)\n",
    "        times.append(np.mean(ptimes).item())\n",
    "        z3_calls.append(np.mean(z3s).item())\n",
    "        ranks.append(np.mean(pranks).item())\n",
    "    return ranks, z3_calls, times\n",
    "\n",
    "def rank_invariants(\n",
    "    fold, \n",
    "    _data, model, cached_embeddings,\n",
    "    which='random',\n",
    "    model_name_addon=\"\",\n",
    "    model_path=None,\n",
    "    num_random_rankings=100,\n",
    "):\n",
    "    print(f\"Ranking {fold} problems with {which}\")\n",
    "    if which == 'gpt':\n",
    "        ranks = []\n",
    "        z3_calls = []\n",
    "        verification_times = []\n",
    "        for di in tqdm(range(len(_data))):\n",
    "            d = _data[di]\n",
    "            z3, inv_count = 0, 0\n",
    "            time_count = 0\n",
    "            peoblem_path = d['problem_path']\n",
    "            for ii in range(len(d['invariants'])):\n",
    "                i = d['invariants'][ii]\n",
    "                z3 += i['call_to_z3']\n",
    "                inv_count += 1\n",
    "                if 'ver_time' in i:\n",
    "                    elapsed_time = i['ver_time']\n",
    "                else:\n",
    "                    start = time.time()\n",
    "                    verify(\n",
    "                        inv=i['code'], \n",
    "                        problem_path=peoblem_path,\n",
    "                    )\n",
    "                    end = time.time()\n",
    "                    elapsed_time = end - start\n",
    "                    _data[di]['invariants'][ii]['ver_time'] = elapsed_time\n",
    "                time_count += elapsed_time\n",
    "                if i['verified']:\n",
    "                    break\n",
    "            z3_calls.append(z3)\n",
    "            ranks.append(inv_count)\n",
    "            verification_times.append(time_count)\n",
    "        return (\n",
    "            ranks, z3_calls, verification_times, \n",
    "            [0] * len(verification_times), \n",
    "            [0] * len(verification_times)\n",
    "        )\n",
    "    elif which == 'similarity':\n",
    "        return similarity_based_ranking(_data)\n",
    "    elif which == 'random':\n",
    "        ranks, z3_calls, verification_times = randomly_rank_invariants(_data, num_random_rankings)\n",
    "        return (\n",
    "            ranks, z3_calls, verification_times, \n",
    "            [0] * len(verification_times), \n",
    "            [0] * len(verification_times)\n",
    "        ) \n",
    "    else:\n",
    "        model_specific_arguments_for_ranker[\n",
    "            \"cached_embeddings\"\n",
    "        ] = cached_embeddings\n",
    "        model_specific_arguments_for_ranker['no_train_rank'] = (which == 'untrained')\n",
    "        new_data = []\n",
    "        for d in _data:\n",
    "            new_data.append({\n",
    "                'problem_path': d['problem_path'],\n",
    "                'problem': d['problem'],\n",
    "                'invariants': [\n",
    "                    {\n",
    "                        'code': i['code'],\n",
    "                        'verified': i['verified'],\n",
    "                        'call_to_z3': i['call_to_z3'],\n",
    "                        'ver_time': i['ver_time'] if 'ver_time' in i else None,\n",
    "                    } for i in d['invariants'] if isinstance(i, dict)\n",
    "                ]\n",
    "            })\n",
    "        if which == 'trained':\n",
    "            if model_path is None:\n",
    "                ckpt = f\"{project_dir}/models/ranker_result/{model_shorthand}{model_name_addon}/mp-2_mn-2_alpha-1/\" +\\\n",
    "                f\"{fold}/checkpoint-best-avg_v_at_1\"\n",
    "            else:\n",
    "                ckpt = model_path\n",
    "            model.load_state_dict(torch.load(\n",
    "                open(f\"{ckpt}/pytorch_model.bin\", 'rb')\n",
    "            ))\n",
    "        model.eval()\n",
    "        torch.no_grad()\n",
    "        ranker = Ranker_new(\n",
    "            data_class=CrossDataSetForCodex, model_class=type(model),\n",
    "            additional_comp_for_ranker=model_specific_arguments_for_ranker,\n",
    "        )\n",
    "        model.cuda()\n",
    "        rank_function = ranker.rank_invariants\n",
    "        ranking_time_start = time.time()\n",
    "        ranked_data = rank_function(\n",
    "            model, new_data, cached_embeddings, bar_tqdm=tqdm\n",
    "        )\n",
    "        ranking_time_end = time.time()\n",
    "        elapsed_ranking_time = ranking_time_end - ranking_time_start\n",
    "        total_invariants = 0\n",
    "        for d in ranked_data:\n",
    "            total_invariants += (1 + len(d['invariants']))\n",
    "        average_elapsed_ranking_time = float(\n",
    "            elapsed_ranking_time) / total_invariants\n",
    "        ranks = []\n",
    "        z3_calls = []\n",
    "        verification_times = []\n",
    "        embedding_times = []\n",
    "        ranking_times = []\n",
    "        for d in ranked_data:\n",
    "            total_code = 1 + len(d['invariants'])\n",
    "            emb_time = avg_embedding_time * total_code\n",
    "            z3 = 0\n",
    "            vtime = 0\n",
    "            ranked_invariants = d['invariants']\n",
    "            for iidx, i in enumerate(ranked_invariants):\n",
    "                vtime += i['ver_time']\n",
    "                z3 += i['call_to_z3']\n",
    "                if i['verified']:\n",
    "                    ranks.append(iidx + 1)\n",
    "                    break\n",
    "            z3_calls.append(z3)\n",
    "            verification_times.append(\n",
    "                vtime\n",
    "            )\n",
    "            embedding_times.append(emb_time)\n",
    "            rtime = average_elapsed_ranking_time * total_code\n",
    "            ranking_times.append(\n",
    "                rtime\n",
    "            )\n",
    "        return ranks, z3_calls, verification_times, embedding_times, ranking_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = {'fold': [], 'experiment': [], 'ranks': [], 'z3_calls': [], 'ver_time': [], 'emb_time': [], 'rank_time': []}\n",
    "\n",
    "for fold in tqdm(['fold_0', 'fold_1', 'fold_2', 'fold_3', 'fold_4']):\n",
    "    fold_1_data = fold_data[fold]\n",
    "    (\n",
    "        gpt_ranks, gpt_z3_calls, gpt_ver_time, \n",
    "        gpt_emb_time, gpt_rank_time\n",
    "    ) = rank_invariants(\n",
    "        fold=fold,\n",
    "        _data=copy(fold_1_data), model=None, \n",
    "        cached_embeddings=cached_embeddings, which='gpt'\n",
    "    )\n",
    "    result_data['fold'].extend([fold] * len(gpt_ranks))\n",
    "    result_data['experiment'].extend(['gpt'] * len(gpt_ranks))\n",
    "    result_data['ranks'].extend(gpt_ranks)\n",
    "    result_data['z3_calls'].extend(gpt_z3_calls)\n",
    "    result_data['ver_time'].extend(gpt_ver_time)\n",
    "    result_data['emb_time'].extend(gpt_emb_time)\n",
    "    result_data['rank_time'].extend(gpt_rank_time)\n",
    "\n",
    "    (\n",
    "        random_ranks, random_z3_calls, random_times,\n",
    "        random_emb_time, random_rank_time\n",
    "    ) = rank_invariants(fold=fold,\n",
    "        _data=copy(fold_1_data), model=None, \n",
    "        cached_embeddings=cached_embeddings, which='random'\n",
    "    )\n",
    "    result_data['fold'].extend([fold] * len(random_ranks))\n",
    "    result_data['experiment'].extend(['random'] * len(random_ranks))\n",
    "    result_data['ranks'].extend(random_ranks)\n",
    "    result_data['z3_calls'].extend(random_z3_calls)\n",
    "    result_data['ver_time'].extend(random_times)\n",
    "    result_data['emb_time'].extend(random_emb_time)\n",
    "    result_data['rank_time'].extend(random_rank_time)\n",
    "\n",
    "    (\n",
    "        sim_ranks, sim_z3_calls, sim_times,\n",
    "        sim_emb_time, sim_rank_time\n",
    "    ) = rank_invariants(fold=fold,\n",
    "        _data=copy(fold_1_data), model=None, \n",
    "        cached_embeddings=cached_embeddings, which='similarity'\n",
    "    )\n",
    "    result_data['fold'].extend([fold] * len(sim_ranks))\n",
    "    result_data['experiment'].extend(['TF-IDF'] * len(sim_ranks))\n",
    "    result_data['ranks'].extend(sim_ranks)\n",
    "    result_data['z3_calls'].extend(sim_z3_calls)\n",
    "    result_data['ver_time'].extend(sim_times)\n",
    "    result_data['emb_time'].extend(sim_emb_time)\n",
    "    result_data['rank_time'].extend(sim_rank_time)\n",
    "\n",
    "    (\n",
    "        untrained_ranks, untrained_z3, untrained_times,\n",
    "        untrained_emb_time, untrained_rank_time\n",
    "    ) = rank_invariants(fold=fold,\n",
    "        _data=copy(fold_1_data), model=model, \n",
    "        cached_embeddings=cached_embeddings, which='untrained'\n",
    "    )\n",
    "    result_data['fold'].extend([fold] * len(untrained_ranks))\n",
    "    result_data['experiment'].extend(['davinci embeddings'] * len(untrained_ranks))\n",
    "    result_data['ranks'].extend(untrained_ranks)\n",
    "    result_data['z3_calls'].extend(untrained_z3)\n",
    "    result_data['ver_time'].extend(untrained_times)\n",
    "    result_data['emb_time'].extend(untrained_emb_time)\n",
    "    result_data['rank_time'].extend(untrained_rank_time)\n",
    "\n",
    "    model_path = get_last_checkpoint(\n",
    "        f\"{project_dir}/models/ranker_result/{model_shorthand}/mp-2_mn-2_alpha-0/\" +\\\n",
    "        f\"{fold}/\") # checkpoint-best-avg_v_at_1\"\n",
    "    ranker_alpha_0, alpha_z3, trained_times, \\\n",
    "        trained_emb_time, trained_rank_time = rank_invariants(fold=fold,\n",
    "        _data=copy(fold_1_data), model=model, \n",
    "        cached_embeddings=cached_embeddings, which='trained',\n",
    "        model_name_addon=\"\",\n",
    "        model_path=model_path\n",
    "    )\n",
    "    result_data['fold'].extend([fold] * len(ranker_alpha_0))\n",
    "    result_data['experiment'].extend(['ranker_binary'] * len(ranker_alpha_0))\n",
    "    result_data['ranks'].extend(ranker_alpha_0)\n",
    "    result_data['z3_calls'].extend(alpha_z3)\n",
    "    result_data['ver_time'].extend(trained_times)\n",
    "    result_data['emb_time'].extend(trained_emb_time)\n",
    "    result_data['rank_time'].extend(trained_rank_time)\n",
    "\n",
    "    df = pd.DataFrame(result_data)\n",
    "    df['total_time'] = df['ver_time'] + df['emb_time'] + df['rank_time']\n",
    "    print_results(df, model=f\"{codex_model}-{fold}-with-time\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.copy()\n",
    "df_['total_time_wo_emb'] = df_['ver_time'] + df_['rank_time']\n",
    "sns.boxplot(\n",
    "    df_, x=\"experiment\", y=\"total_time_wo_emb\", showfliers=False\n",
    ")\n",
    "plt.ylabel(\"Ranking + Verification\")\n",
    "plt.show()\n",
    "\n",
    "df_['total_time'] = df_['ver_time'] + df_['rank_time'] + df_['emb_time']\n",
    "sns.boxplot(\n",
    "    df_, x=\"experiment\", y=\"total_time\", showfliers=False\n",
    ")\n",
    "plt.ylabel(\"Emb + Ranking + Verification\")\n",
    "plt.show()\n",
    "print(df_['total_time'].mean(), df_['total_time_wo_emb'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "    \n",
    "def draw_graphs(\n",
    "        exp_name, total, save_file_name, \n",
    "        folder, input_folder=\"aggregated_results\", \n",
    "        ext1=\"\", ext2=\"\", compute_time=False\n",
    "):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    df1 = pd.read_csv(\n",
    "        f\"./{input_folder}/davinci-similarity-fold_4{ext1}\" +\\\n",
    "            f\"-{exp_name}{ext2}-results.csv\", index_col=0\n",
    "    )\n",
    "    df2 = pd.read_csv(\n",
    "        f\"./{input_folder}/text-embedding-ada-002-fold_4{ext1}\" +\\\n",
    "            f\"-{exp_name}{ext2}-results.csv\", index_col=0\n",
    "    )\n",
    "    gpt_result = df1[df1.experiment == \"gpt\"]\n",
    "    gpt_result.experiment = \"LLM Ranks\"\n",
    "    random_result = df2[df2.experiment == \"random\"]\n",
    "    random_result.experiment = \"Expected Ranks\"\n",
    "    davinci_result = df1[df1.experiment == \"davinci embeddings\"]\n",
    "    davinci_result.experiment = \"Davinci emb.\"\n",
    "    ada_result = df2[df2.experiment == \"davinci embeddings\"]\n",
    "    ada_result.experiment = \"Ada emb.\"\n",
    "    ranker_result = df1[df1.experiment == \"ranker_binary\"]\n",
    "    ranker_result.experiment = \"iRank-Davinci\"\n",
    "    ranker_result_ada = df2[df2.experiment == \"ranker_binary\"]\n",
    "    ranker_result_ada.experiment = \"iRank-Ada\"\n",
    "    tf_idf_result = df2[df2.experiment == \"TF-IDF\"]\n",
    "    df = pd.concat(\n",
    "        [gpt_result, random_result, tf_idf_result,\n",
    "        davinci_result, ada_result, \n",
    "        ranker_result_ada, ranker_result]\n",
    "    )\n",
    "    df['z3_calls'] = df['z3_calls'].astype(int) - 1\n",
    "    print(df['experiment'].unique())\n",
    "\n",
    "    d = df.copy()\n",
    "    del d['fold']\n",
    "    del d['z3_calls']\n",
    "    if compute_time:\n",
    "        del d['ver_time']\n",
    "        del d['emb_time']\n",
    "        del d['rank_time']\n",
    "        del d['total_time']\n",
    "    grouped_df = d.groupby('experiment')\n",
    "    mean = grouped_df.mean()\n",
    "    mean = mean.rename(columns={'ranks': 'mean'})\n",
    "    median = grouped_df.median()\n",
    "    median = median.rename(columns={'ranks': 'median'})\n",
    "    x = pd.concat([mean, median], axis=1)\n",
    "    x = x.reset_index()\n",
    "    x = x.rename(columns={'experiment': 'Experiment'})\n",
    "    print(\"================== Ranks ===================\")\n",
    "    print(x)\n",
    "\n",
    "    if compute_time:\n",
    "        print(\"================== Embedding Time ===================\")\n",
    "        d = df.copy()\n",
    "        del d['fold']\n",
    "        del d['z3_calls']\n",
    "        del d['ranks']\n",
    "        del d['ver_time']\n",
    "        del d['rank_time']\n",
    "        del d['total_time']\n",
    "        grouped_df = d.groupby('experiment')\n",
    "        mean = grouped_df.mean()\n",
    "        mean = mean.rename(columns={'emb_time': 'mean'})\n",
    "        median = grouped_df.median()\n",
    "        median = median.rename(columns={'emb_time': 'median'})\n",
    "        x = pd.concat([mean, median], axis=1)\n",
    "        x = x.reset_index()\n",
    "        x = x.rename(columns={'experiment': 'Experiment'})\n",
    "        print(x)\n",
    "            \n",
    "        print(\"================== Ranking Time ===================\")\n",
    "        d = df.copy()\n",
    "        del d['fold']\n",
    "        del d['z3_calls']\n",
    "        del d['ranks']\n",
    "        del d['emb_time']\n",
    "        del d['ver_time']\n",
    "        del d['total_time']\n",
    "        grouped_df = d.groupby('experiment')\n",
    "        mean = grouped_df.mean()\n",
    "        mean = mean.rename(columns={'rank_time': 'mean'})\n",
    "        median = grouped_df.median()\n",
    "        median = median.rename(columns={'rank_time': 'median'})\n",
    "        x = pd.concat([mean, median], axis=1)\n",
    "        x = x.reset_index()\n",
    "        x = x.rename(columns={'experiment': 'Experiment'})\n",
    "        print(x)\n",
    "        print(\"===============================================\")\n",
    "\n",
    "        print(\"================== Verification Time ===================\")\n",
    "        d = df.copy()\n",
    "        del d['fold']\n",
    "        del d['z3_calls']\n",
    "        del d['ranks']\n",
    "        del d['emb_time']\n",
    "        del d['rank_time']\n",
    "        del d['total_time']\n",
    "        grouped_df = d.groupby('experiment')\n",
    "        mean = grouped_df.mean()\n",
    "        mean = mean.rename(columns={'ver_time': 'mean'})\n",
    "        median = grouped_df.median()\n",
    "        median = median.rename(columns={'ver_time': 'median'})\n",
    "        x = pd.concat([mean, median], axis=1)\n",
    "        x = x.reset_index()\n",
    "        x = x.rename(columns={'experiment': 'Experiment'})\n",
    "        print(x)\n",
    "            \n",
    "        print(\"================== Total Time ===================\")\n",
    "        d = df.copy()\n",
    "        del d['fold']\n",
    "        del d['z3_calls']\n",
    "        del d['ranks']\n",
    "        del d['emb_time']\n",
    "        del d['rank_time']\n",
    "        del d['ver_time']\n",
    "        grouped_df = d.groupby('experiment')\n",
    "        mean = grouped_df.mean()\n",
    "        mean = mean.rename(columns={'total_time': 'mean'})\n",
    "        median = grouped_df.median()\n",
    "        median = median.rename(columns={'total_time': 'median'})\n",
    "        x = pd.concat([mean, median], axis=1)\n",
    "        x = x.reset_index()\n",
    "        x = x.rename(columns={'experiment': 'Experiment'})\n",
    "        print(x)\n",
    "\n",
    "\n",
    "    hatches = ['--', '++', 'xx', '\\\\\\\\\\\\', '**', 'oo', '...', \"///\"]\n",
    "    #  Ranks\n",
    "    plt.figure(figsize=(4.5, 3.5))\n",
    "    ax = sns.boxplot(\n",
    "        y=\"ranks\", x='experiment', data=df, \n",
    "            showfliers=False,)\n",
    "    patches = [patch for patch in ax.patches if type(patch) == mpl.patches.PathPatch]\n",
    "    for patch, hatch in zip(patches, hatches):\n",
    "        patch.set_hatch(hatch)\n",
    "    plt.xlabel(\"Experiments\")\n",
    "    plt.ylabel(\"Rank of the verified invariant\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.savefig(f\"{folder}/{save_file_name}-ranks.pdf\", bbox_inches='tight')\n",
    "\n",
    "    # Z3 calls\n",
    "    plt.figure(figsize=(4.5, 3.5))\n",
    "    ax = sns.boxplot(\n",
    "        y=\"z3_calls\", x='experiment', data=df, \n",
    "            showfliers=False,)\n",
    "    patches = [patch for patch in ax.patches if type(patch) == mpl.patches.PathPatch]\n",
    "    for patch, hatch in zip(patches, hatches):\n",
    "        patch.set_hatch(hatch)\n",
    "    plt.xlabel(\"Experiments\")\n",
    "    plt.ylabel(\"Number of Z3 calls\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.savefig(f\"{folder}/{save_file_name}-z3_calls.pdf\", bbox_inches='tight')\n",
    "\n",
    "    if compute_time:\n",
    "        # Verification time\n",
    "        plt.figure(figsize=(4.5, 3.5))\n",
    "        ax = sns.boxplot(\n",
    "            y=\"ver_time\", x='experiment', data=df,\n",
    "                showfliers=False,)\n",
    "        patches = [patch for patch in ax.patches if type(patch) == mpl.patches.PathPatch]\n",
    "        for patch, hatch in zip(patches, hatches):\n",
    "            patch.set_hatch(hatch)\n",
    "        plt.xlabel(\"Experiments\")\n",
    "        plt.ylabel(\"Verification time (s)\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.savefig(f\"{folder}/{save_file_name}-ver_time.pdf\", bbox_inches='tight')\n",
    "\n",
    "        # Total time\n",
    "        plt.figure(figsize=(4.5, 3.5))\n",
    "        ax = sns.boxplot(\n",
    "            y=\"total_time\", x='experiment', data=df,\n",
    "                showfliers=False,)\n",
    "        patches = [patch for patch in ax.patches if type(patch) == mpl.patches.PathPatch]\n",
    "        for patch, hatch in zip(patches, hatches):\n",
    "            patch.set_hatch(hatch)\n",
    "        plt.xlabel(\"Experiments\")\n",
    "        plt.ylabel(\"Total time (s)\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.savefig(f\"{folder}/{save_file_name}-total_time.pdf\", bbox_inches='tight')\n",
    "\n",
    "    results = {}\n",
    "    data = df.to_dict('records')\n",
    "    max_ranks = 50\n",
    "    for d in data:\n",
    "        exp = d['experiment']\n",
    "        rank = int(d['ranks'])\n",
    "        if d['experiment'] not in results:\n",
    "            results[exp] = {k: 0 for k in range(1, max_ranks + 1)}\n",
    "        for k in range(rank, max_ranks + 1):\n",
    "            results[exp][k] += 1   \n",
    "    result_df = {\n",
    "        \"experiment\": [],\n",
    "        \"rank\": [],\n",
    "        \"# Verified\": []\n",
    "    } \n",
    "    for exp in results:\n",
    "        for rank in results[exp]:\n",
    "            result_df[\"experiment\"].append(exp)\n",
    "            result_df[\"rank\"].append(int(rank))\n",
    "            result_df[\"# Verified\"].append(float(results[exp][rank]) * 100 / total)\n",
    "    result_df = pd.DataFrame(result_df)\n",
    "    plt.figure(figsize=(5.2, 3.5))\n",
    "    # Increase the font size for the legend text and axes labels\n",
    "    plt.rcParams['legend.fontsize'] = 12\n",
    "    plt.rcParams['axes.labelsize'] = 12\n",
    "    plt.rcParams['axes.titlesize'] = 12\n",
    "    plt.rcParams['xtick.labelsize'] = 12\n",
    "    plt.rcParams['ytick.labelsize'] = 12\n",
    "    plt.rcParams['font.size'] = 12\n",
    "    sns.lineplot(\n",
    "        data=result_df, x=\"rank\", y=\"# Verified\",\n",
    "        hue=\"experiment\", palette=\"colorblind\",\n",
    "        style=\"experiment\", \n",
    "        dashes=True,\n",
    "        linewidth=2.1,\n",
    "    )\n",
    "    plt.ylabel(\"Percentage of Problems Verified\")\n",
    "    plt.xlabel(\"Top k Invariants Tried\")\n",
    "    plt.legend(ncol=2, loc='upper center', \n",
    "               bbox_to_anchor=(0.48, 1.5))\n",
    "    plt.savefig(f\"{folder}/{save_file_name}-lineplot.pdf\", bbox_inches='tight')\n",
    "    print(\"=====================================\")\n",
    "    vdf_1 = result_df[result_df[\"rank\"] == 1]\n",
    "    vdf_2 = result_df[result_df[\"rank\"] == 2]\n",
    "    vdf_5 = result_df[result_df[\"rank\"] == 5]\n",
    "    vdf_10 = result_df[result_df[\"rank\"] == 10]\n",
    "    complete = pd.concat([vdf_1, vdf_2, vdf_5, vdf_10], axis=0).reset_index(drop=True)\n",
    "    print(complete.groupby([\"experiment\", \"rank\"]).mean())\n",
    "    print(\"=====================================\")\n",
    "\n",
    "    if compute_time:\n",
    "        time_ticks = [0.25 * x for x in list(range(70))]\n",
    "        results_time = {}\n",
    "        data_time = df.to_dict('records')\n",
    "        for d in data_time:\n",
    "            exp = d['experiment']\n",
    "            time = float(d['total_time'])\n",
    "            if exp not in results_time:\n",
    "                results_time[exp] = {\n",
    "                    ki: 0 for ki, k in enumerate(time_ticks)\n",
    "                }\n",
    "            assert exp in results_time\n",
    "            for ki, k in enumerate(time_ticks):\n",
    "                if k >= time:\n",
    "                    results_time[exp][ki] += 1   \n",
    "        result_df = {\n",
    "            \"experiment\": [],\n",
    "            \"time\": [],\n",
    "            \"# Verified\": []\n",
    "        } \n",
    "        for exp in results_time.keys():\n",
    "            for t in results_time[exp].keys():\n",
    "                # print(type(results_time[exp][t]), type(total))\n",
    "                result_df[\"experiment\"].append(exp)\n",
    "                result_df[\"time\"].append(int(t))\n",
    "                result_df[\"# Verified\"].append(\n",
    "                    float(results_time[exp][t]) * 100 / total\n",
    "                )\n",
    "        result_df = pd.DataFrame(result_df)\n",
    "        fig, ax = plt.subplots(figsize=(5.2, 3.5))\n",
    "        plt.rcParams['legend.fontsize'] = 12\n",
    "        plt.rcParams['axes.labelsize'] = 12\n",
    "        plt.rcParams['axes.titlesize'] = 12\n",
    "        plt.rcParams['xtick.labelsize'] = 12\n",
    "        plt.rcParams['ytick.labelsize'] = 12\n",
    "        plt.rcParams['font.size'] = 12\n",
    "        sns.lineplot(\n",
    "            data=result_df, x=\"time\", y=\"# Verified\",\n",
    "            hue=\"experiment\", palette=\"colorblind\",\n",
    "            style=\"experiment\", \n",
    "            dashes=True,\n",
    "            linewidth=2.1,\n",
    "        )\n",
    "        ticks = list(range(0, len(time_ticks), 10))\n",
    "        labels = [time_ticks[t] for t in ticks]\n",
    "        plt.xticks(ticks, labels)\n",
    "        plt.ylabel(\"Percentage of Problems Verified\")\n",
    "        plt.xlabel(\"Total Time (s)\")\n",
    "        plt.legend(ncol=2, loc='upper center', \n",
    "                bbox_to_anchor=(0.48, 1.5))\n",
    "        plt.savefig(f\"{folder}/{save_file_name}-lineplot-total-time.pdf\", bbox_inches='tight')\n",
    "\n",
    "    \n",
    "draw_graphs(\n",
    "    exp_name=\"gpt-3.5\",\n",
    "    total=250,\n",
    "    save_file_name=\"gpt3.5\",\n",
    "    folder=\"results_with_time_and_tfidf\",\n",
    "    input_folder=\"aggregated_results_with_time\",\n",
    "    ext1=\"-with-time\", compute_time=True\n",
    ")\n",
    "\n",
    "draw_graphs(\n",
    "    exp_name=\"gpt-4\",\n",
    "    total=188,\n",
    "    save_file_name=\"gpt4\",\n",
    "    folder=\"results_with_time_and_tfidf\",\n",
    "    input_folder=\"aggregated_results_with_time\",\n",
    "    ext1=\"-with-time\", compute_time=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm4li",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
